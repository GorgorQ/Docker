1-1 For which reason is it better to run the container with a flag -e to give the environment variables 
rather than put them directly in the Dockerfile?

Becaause without the -e flag, the credentials are visible for anyone having the image. 
They only need to do tis comand : docker history my-image, to get the env variables in plain text, 
which is a major security issue.

1-2 Why do we need a volume to be attached to our postgres container?

We need a volume attached to our postgres container for data persistence. Without a volume, all data 
stored in the database is kept only in the container's writable layer. When the container is stopped 
or destroyed (docker rm), all the data is permanently lost.

1-3 Document your database container essentials: commands and Dockerfile.

Dockerfile:
FROM postgres:17.2-alpine
COPY 01-CreateScheme.sql /docker-entrypoint-initdb.d/
COPY 02-InsertData.sql /docker-entrypoint-initdb.d/

Essential Commands:
- Create network: docker network create app-network
- Create volume: docker volume create postgres-data
- Build image: docker build -t my-postgres-db .
- Run container: docker run --name postgres-container --network app-network --env-file .env 
  -v postgres-data:/var/lib/postgresql/data -d my-postgres-db
- Run Adminer: docker run -p "8090:8080" --net=app-network --name=adminer -d adminer

The Dockerfile copies SQL initialization scripts that are automatically executed on first startup.
Environment variables (POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD) are passed via .env file for security.

1-4 Why do we need a multistage build? And explain each step of this dockerfile.

We need a multistage build to reduce the final image size and improve security. The first stage uses 
a JDK image with Maven to compile the Java source code into a JAR file. The second stage uses only a 
JRE image and copies the compiled JAR from the build stage, excluding all build tools and source code. 
This results in a much smaller production image with fewer components, 
reducing the attack surface and eliminating the need for Maven and JDK on the host machine.


Dockerfile explanation:

BUILD STAGE:
FROM eclipse-temurin:21-jdk-alpine AS myapp-build
  -> Uses JDK image for compilation, names this stage "myapp-build"
ENV MYAPP_HOME=/opt/myapp
  -> Sets environment variable for application directory
WORKDIR $MYAPP_HOME
  -> Sets working directory to /opt/myapp
RUN apk add --no-cache maven
  -> Installs Maven package manager (Alpine uses apk)
COPY pom.xml . and COPY src ./src
  -> Copies project configuration and source code
RUN mvn package -DskipTests
  -> Compiles the application into a JAR file in target/ directory

RUN STAGE:
FROM eclipse-temurin:21-jre-alpine
  -> New stage with JRE only (no JDK, no Maven) - much smaller
ENV MYAPP_HOME=/opt/myapp and WORKDIR $MYAPP_HOME
  -> Sets same working directory
COPY --from=myapp-build $MYAPP_HOME/target/*.jar $MYAPP_HOME/myapp.jar
  -> Copies ONLY the compiled JAR from build stage (not source code or build tools)
ENTRYPOINT ["java", "-jar", "myapp.jar"]
  -> Defines the command to run when container starts

Result: Final image contains only JRE + JAR file, reducing size from ~500MB to ~200MB.

1-5 Why do we need a reverse proxy?

A reverse proxy serves as a single entry point for client requests, providing multiple benefits for our 
application architecture. It hides the backend infrastructure by exposing only port 80 to the public 
while keeping the application server (port 8080) internal. This improves security, enables load balancing 
across multiple backend instances, handles SSL/TLS encryption centrally, and can cache static content 
for better performance. In our case, Apache httpd forwards all incoming requests to the Spring Boot API, 
creating a clean separation between the web server layer and the application layer.

┌─────────────────────────────────────────┐
│   Client (navigateur/curl)              │
└──────────────┬──────────────────────────┘
               │ Port 80
┌──────────────▼──────────────────────────┐
│   httpd (Apache Reverse Proxy)          │
│   - Reçoit les requêtes HTTP            │
│   - Proxifie vers backend:8080          │
└──────────────┬──────────────────────────┘
               │ app-network
┌──────────────▼──────────────────────────┐
│   backend (Spring Boot + Java 21)       │
│   - API REST                            │
│   - Se connecte à la DB                 │
└──────────────┬──────────────────────────┘
               │ app-network
┌──────────────▼──────────────────────────┐
│   database (PostgreSQL 17)              │
│   - Stocke les données                  │
│   - Volume persistant                   │
└─────────────────────────────────────────┘


1-6 Why is docker-compose so important?

Docker-compose is important because it enables the launching of multicontainers applications with only 1 
command.

1-7 Document docker-compose most important commands.

- docker-compose up: Builds, creates, and starts all services defined in docker-compose.yml
- docker-compose up -d: Starts services in detached mode (background)
- docker-compose up --build: Forces rebuild of images before starting
- docker-compose down: Stops and removes all containers, networks (but preserves volumes)


1-8 Document your docker-compose file.

The docker-compose.yml orchestrates a 3-tier application with the following services:

backend-api (Spring Boot API):
- Builds from ../Backend API folder
- Loads environment variables from ../.env file
- Overrides POSTGRES_HOST=database to use Docker service name
- Connects to app-network for inter-container communication
- Waits for database to be healthy before starting
- Has healthcheck using curl on /actuator/health endpoint
- Checks health every 10s with 30s grace period for startup

database (PostgreSQL):
- Builds from ../Database folder with initialization SQL scripts
- Loads credentials from ../.env (POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD)
- Connects to app-network
- Uses postgres-data volume for data persistence
- Has healthcheck using pg_isready command

httpd (Apache Reverse Proxy):
- Builds from ../Http server folder with reverse proxy configuration
- Exposes port 80 to host (only public entry point)
- Connects to app-network
- Waits for backend-api to be healthy before starting
- Proxies all requests to backend-api:8080

Networks:
- app-network: Isolated network enabling DNS-based service discovery

Volumes:
- postgres-data: Persistent storage for database files

Startup order: database → backend-api → httpd (enforced by depends_on with healthcheck conditions)


1-9 Document your publication commands and published images in dockerhub.

Publication Commands:

1. Login to Docker Hub:
  -docker login
  -(Enter username: gorgorq and password)

2. Tag images with version 1.0:
  -docker tag my-postgres-db gorgorq/my-database:1.0
  -docker tag simple-api gorgorq/simple-api:1.0
  -docker tag my-httpd gorgorq/my-httpd:1.0

3. Push images to Docker Hub:
  -docker push gorgorq/my-database:1.0
  -docker push gorgorq/simple-api:1.0
  -docker push gorgorq/my-httpd:1.0


1-10 Why do we put our images into an online repo?


We put our images on an online repo to enable anybody to download and use them without needing to build 
them locally. 

It enables easy deployment to production servers, cloud platforms, or any Docker host with 
a simple docker pull command. We can maintain different versions of images with proper tagging (1.0, 1.1, 
latest...) which allows version control and rollback capability. 

It also facilitates team collaboration 
by ensuring everyone uses the same consistent images across different environments. 

The online repo serves 
as a backup if local images are lost and integrates well with CI/CD pipelines for automated deployments. 
Users don't need access to source code, Dockerfiles, or build tools to run the application.

2-1 What are testcontainers?

They simply are java libraries that allow you to run a bunch of docker containers while testing. 
Here we use the postgresql container to attach to our application while testing



2-2 For what purpose do we need to use secured variables ?

We need secured variables (GitHub Secrets) to protect sensitive credentials like Docker Hub username and 
tokens. Publishing credentials directly in code or configuration files would expose them to anyone with 
repository access, creating major security risks.

2-3 Why did we put needs: build-and-test-backend on this job? Maybe try without this and you will see!

The needs: test-backend directive ensures that docker images are only built if the tests pass successfully. 
Without this dependency, images could be built and pushed even if the code has failing tests or compilation 
errors, potentially deploying broken code to production. This creates a safety gate: if mvn clean verify 
fails, the entire workflow stops and no images are published, maintaining code quality standards.


2-4 For what purpose do we need to push docker images?

We need to push Docker images to Docker Hub to make them accessible for deployment across different 
environments and teams. Pushing images enables easy deployment to production servers, staging environments, 
or any Docker host with a simple docker pull command without needing to rebuild. It provides version 
control through tags (latest, 1.0, 1.1...) allowing rollback to previous versions if needed. The registry 
serves as a centralized storage and backup for images, ensuring they're available even if local copies 
are lost. It facilitates team collaboration by ensuring everyone uses the same tested and validated images. 
Most importantly, it's essential for CI/CD pipelines where automated deployments fetch the latest tested 
images from the registry. Users can run the application without access to source code, Dockerfiles, or 
build tools, they only need docker pull and docker run commands.


3-1 Document your inventory and base commands

Inventory Structure (inventories/setup.yml):
The inventory file defines target servers and connection parameters using YAML format. It contains 'all' 
group with common variables (ansible_user: admin, ansible_ssh_private_key_file: ~/.ssh/id_rsa) and 'prod' 
group listing alexandre.kalaydjian.takima.cloud as the production host. Groups enable targeted deployments 
and environment separation.

Base Commands:
- ansible all -i inventories/setup.yml -m ping: Tests SSH connectivity
- ansible all -i inventories/setup.yml -m setup: Gathers system information
- ansible all -i inventories/setup.yml -m command -a "uptime": Executes shell commands
- ansible all -i inventories/setup.yml -m apt -a "name=docker.io state=present" --become: Installs packages
- ansible-playbook -i inventories/setup.yml playbook.yml: Runs automated playbooks

Creating ansible.cfg with "inventory = inventories/setup.yml" simplifies commands by removing the -i flag. 
Ansible connects via SSH using the private key, executes commands remotely through Python, and requires 
no installation on target servers.

3-2 Document your playbook

The playbook orchestrates Docker installation on production servers through a role-based architecture:

```yaml
- hosts: all
  gather_facts: true
  become: true
  roles:
    - docker
```

- hosts: all - Targets all hosts defined in inventories/setup.yml (alexandre.kalaydjian.takima.cloud)
- gather_facts: true - Collects system information (OS version, architecture, Python path) used by tasks 
  for intelligent decisions like detecting ansible_facts['distribution_release'] for Debian version
- become: true - Executes all tasks with sudo privileges, required for system-level operations
- roles: - docker - Calls the docker role containing modular tasks in roles/docker/tasks/main.yml

The docker role installs prerequisites (apt-transport-https, curl, gnupg), adds Docker's GPG key and 
repository, installs Docker CE, sets up Python3 with Docker SDK in a virtual environment at /opt/docker_venv, 
and ensures the Docker service is running and enabled at boot. Handlers in roles/docker/handlers/main.yml 
respond to configuration changes by restarting Docker when notified. This role-based structure keeps the 
main playbook concise (5 lines) while maintaining detailed installation logic separately for reusability 
across projects, following Ansible best practices for Infrastructure as Code with version-controlled, 
repeatable, and testable deployments.

Execution: ansible-playbook -i inventories/setup.yml playbook.yml
Verification: ansible all -i inventories/setup.yml -m command -a "docker --version"

