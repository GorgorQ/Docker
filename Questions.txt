1-1 For which reason is it better to run the container with a flag -e to give the environment variables 
rather than put them directly in the Dockerfile?

Becaause without the -e flag, the credentials are visible for anyone having the image. 
They only need to do tis comand : docker history my-image, to get the env variables in plain text, 
which is a major security issue.

1-2 Why do we need a volume to be attached to our postgres container?

We need a volume attached to our postgres container for data persistence. Without a volume, all data 
stored in the database is kept only in the container's writable layer. When the container is stopped 
or destroyed (docker rm), all the data is permanently lost.

1-3 Document your database container essentials: commands and Dockerfile.

Dockerfile:
FROM postgres:17.2-alpine
COPY 01-CreateScheme.sql /docker-entrypoint-initdb.d/
COPY 02-InsertData.sql /docker-entrypoint-initdb.d/

Essential Commands:
- Create network: docker network create app-network
- Create volume: docker volume create postgres-data
- Build image: docker build -t my-postgres-db .
- Run container: docker run --name postgres-container --network app-network --env-file .env 
  -v postgres-data:/var/lib/postgresql/data -d my-postgres-db
- Run Adminer: docker run -p "8090:8080" --net=app-network --name=adminer -d adminer

The Dockerfile copies SQL initialization scripts that are automatically executed on first startup.
Environment variables (POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD) are passed via .env file for security.

1-4 Why do we need a multistage build? And explain each step of this dockerfile.

We need a multistage build to reduce the final image size and improve security. The first stage uses 
a JDK image with Maven to compile the Java source code into a JAR file. The second stage uses only a 
JRE image and copies the compiled JAR from the build stage, excluding all build tools and source code. 
This results in a much smaller production image with fewer components, 
reducing the attack surface and eliminating the need for Maven and JDK on the host machine.


Dockerfile explanation:

BUILD STAGE:
FROM eclipse-temurin:21-jdk-alpine AS myapp-build
  -> Uses JDK image for compilation, names this stage "myapp-build"
ENV MYAPP_HOME=/opt/myapp
  -> Sets environment variable for application directory
WORKDIR $MYAPP_HOME
  -> Sets working directory to /opt/myapp
RUN apk add --no-cache maven
  -> Installs Maven package manager (Alpine uses apk)
COPY pom.xml . and COPY src ./src
  -> Copies project configuration and source code
RUN mvn package -DskipTests
  -> Compiles the application into a JAR file in target/ directory

RUN STAGE:
FROM eclipse-temurin:21-jre-alpine
  -> New stage with JRE only (no JDK, no Maven) - much smaller
ENV MYAPP_HOME=/opt/myapp and WORKDIR $MYAPP_HOME
  -> Sets same working directory
COPY --from=myapp-build $MYAPP_HOME/target/*.jar $MYAPP_HOME/myapp.jar
  -> Copies ONLY the compiled JAR from build stage (not source code or build tools)
ENTRYPOINT ["java", "-jar", "myapp.jar"]
  -> Defines the command to run when container starts

Result: Final image contains only JRE + JAR file, reducing size from ~500MB to ~200MB.

1-5 Why do we need a reverse proxy?

A reverse proxy serves as a single entry point for client requests, providing multiple benefits for our 
application architecture. It hides the backend infrastructure by exposing only port 80 to the public 
while keeping the application server (port 8080) internal. This improves security, enables load balancing 
across multiple backend instances, handles SSL/TLS encryption centrally, and can cache static content 
for better performance. In our case, Apache httpd forwards all incoming requests to the Spring Boot API, 
creating a clean separation between the web server layer and the application layer.

┌─────────────────────────────────────────┐
│   Client (navigateur/curl)              │
└──────────────┬──────────────────────────┘
               │ Port 80
┌──────────────▼──────────────────────────┐
│   httpd (Apache Reverse Proxy)          │
│   - Reçoit les requêtes HTTP            │
│   - Proxifie vers backend:8080          │
└──────────────┬──────────────────────────┘
               │ app-network
┌──────────────▼──────────────────────────┐
│   backend (Spring Boot + Java 21)       │
│   - API REST                            │
│   - Se connecte à la DB                 │
└──────────────┬──────────────────────────┘
               │ app-network
┌──────────────▼──────────────────────────┐
│   database (PostgreSQL 17)              │
│   - Stocke les données                  │
│   - Volume persistant                   │
└─────────────────────────────────────────┘


1-6 Why is docker-compose so important?

Docker-compose is important because it enables the launching of multicontainers applications with only 1 
command.

1-7 Document docker-compose most important commands.

- docker-compose up: Builds, creates, and starts all services defined in docker-compose.yml
- docker-compose up -d: Starts services in detached mode (background)
- docker-compose up --build: Forces rebuild of images before starting
- docker-compose down: Stops and removes all containers, networks (but preserves volumes)


1-8 Document your docker-compose file.

The docker-compose.yml orchestrates a 3-tier application with the following services:

backend-api (Spring Boot API):
- Builds from ../Backend API folder
- Loads environment variables from ../.env file
- Overrides POSTGRES_HOST=database to use Docker service name
- Connects to app-network for inter-container communication
- Waits for database to be healthy before starting
- Has healthcheck using curl on /actuator/health endpoint
- Checks health every 10s with 30s grace period for startup

database (PostgreSQL):
- Builds from ../Database folder with initialization SQL scripts
- Loads credentials from ../.env (POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD)
- Connects to app-network
- Uses postgres-data volume for data persistence
- Has healthcheck using pg_isready command

httpd (Apache Reverse Proxy):
- Builds from ../Http server folder with reverse proxy configuration
- Exposes port 80 to host (only public entry point)
- Connects to app-network
- Waits for backend-api to be healthy before starting
- Proxies all requests to backend-api:8080

Networks:
- app-network: Isolated network enabling DNS-based service discovery

Volumes:
- postgres-data: Persistent storage for database files

Startup order: database → backend-api → httpd (enforced by depends_on with healthcheck conditions)


1-9 Document your publication commands and published images in dockerhub.

Publication Commands:

1. Login to Docker Hub:
  -docker login
  -(Enter username: gorgorq and password)

2. Tag images with version 1.0:
  -docker tag my-postgres-db gorgorq/my-database:1.0
  -docker tag simple-api gorgorq/simple-api:1.0
  -docker tag my-httpd gorgorq/my-httpd:1.0

3. Push images to Docker Hub:
  -docker push gorgorq/my-database:1.0
  -docker push gorgorq/simple-api:1.0
  -docker push gorgorq/my-httpd:1.0


1-10 Why do we put our images into an online repo?


We put our images on an online repo to enable anybody to download and use them without needing to build 
them locally. 

It enables easy deployment to production servers, cloud platforms, or any Docker host with 
a simple docker pull command. We can maintain different versions of images with proper tagging (1.0, 1.1, 
latest...) which allows version control and rollback capability. 

It also facilitates team collaboration 
by ensuring everyone uses the same consistent images across different environments. 

The online repo serves 
as a backup if local images are lost and integrates well with CI/CD pipelines for automated deployments. 
Users don't need access to source code, Dockerfiles, or build tools to run the application.

2-1 What are testcontainers?

They simply are java libraries that allow you to run a bunch of docker containers while testing. 
Here we use the postgresql container to attach to our application while testing